{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1,12,1)\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0])\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.5,0.5,0.5,0.5,0.5, 0.5,0.5,0.5,0.5,0.5,0.5])\n",
    "w, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = X @ w.T\n",
    "weight # positive means the x point is in class 1 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bar = np.array([-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11])\n",
    "X_bar, X_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = X_bar @ w.T \n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is an essential technique in logistic regression that helps prevent overfitting and improves the generalization ability of the model. Here are the key reasons why regularization is important in logistic regression:\n",
    "\n",
    "1. **Preventing Overfitting**: Logistic regression models with a large number of features or complex relationships between features and the target variable are prone to overfitting. Overfitting occurs when the model learns the training data too well and fails to generalize to new, unseen data. Regularization techniques, such as L1 (Lasso) and L2 (Ridge) regularization, introduce a penalty term to the loss function, which discourages the model from assigning excessive importance to any particular feature. This helps control the complexity of the model and reduces overfitting.\n",
    "\n",
    "2. **Feature Selection**: Regularization techniques can drive the coefficients of irrelevant or less important features towards zero. This effectively performs feature selection by shrinking the coefficients of less informative features. By reducing the impact of irrelevant features, regularization helps to focus the model on the most relevant features, improving interpretability and reducing the risk of overfitting.\n",
    "\n",
    "3. **Improving Model Stability**: Regularization helps stabilize the logistic regression model by reducing the sensitivity to small changes in the input data. When the model is regularized, the coefficients are constrained, which makes the model less sensitive to noise or outliers in the data. This improves the stability of the model's predictions and makes it more robust.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Regularization plays a crucial role in managing the bias-variance tradeoff. By adding a penalty term to the loss function, regularization helps strike a balance between the model's ability to fit the training data (low bias) and its ability to generalize to new data (low variance). It prevents the model from becoming too complex and overly fitting the training data, which can lead to high variance and poor generalization.\n",
    "\n",
    "In summary, regularization is important in logistic regression because it helps prevent overfitting, performs feature selection, improves model stability, and manages the bias-variance tradeoff. By incorporating regularization techniques, logistic regression models can achieve better performance and generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inp):\n",
    "    return 1/1 + np.exp(-inp)\n",
    "\n",
    "sigmoid(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression function\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Toy dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "def plot(X, w, b, y):\n",
    "    # Plot the data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    x_boundary = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    y_boundary = -(w[0] * x_boundary + b) / w[1]\n",
    "    plt.plot(x_boundary, y_boundary, color='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic regression function\n",
    "def logistic_regression(X, y, num_iterations, learning_rate):\n",
    "    # Initialize weights and bias\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    # Gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        # Linear combination of weights and features\n",
    "        z = np.dot(X, w) + b\n",
    "\n",
    "        # Apply sigmoid function\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # Calculate gradients\n",
    "        dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "        db = (1 / num_samples) * np.sum(y_pred - y)\n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Train logistic regression model\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "w, b = logistic_regression(X, y, num_iterations, learning_rate)\n",
    "plot(X, w, b, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_function(x):\n",
    "    return x ** 2 \n",
    "\n",
    "def y_derivative(x):\n",
    "    return 2 * x \n",
    "\n",
    "def gradient_descent(X, y , lr = 0.1, thresh=0.01 , iter = 50):\n",
    "    \n",
    "    current_pos = ( 50, y_function(50))\n",
    "    \n",
    "    for _ in range(iter):\n",
    "        grad = y_derivative(current_pos[0])\n",
    "        x_new = current_pos[0] - (lr * grad)\n",
    "        y_new = y_function(x_new)\n",
    "        current_pos = ( x_new, y_new)\n",
    "        plt.plot(X,y)\n",
    "        plt.scatter(current_pos[0], current_pos[1], color='red')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "X = np.arange(-100, 100, 0.1)\n",
    "y = y_function(X)\n",
    "plt.plot(X, y)\n",
    "plt.show()\n",
    "\n",
    "gradient_descent(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
